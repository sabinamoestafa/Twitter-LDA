# -*- coding: utf-8 -*-
"""Twitter LDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12P0j0j53wwxSzj_BD-nWydM85ZjspfP1
"""

!curl -sL https://deb.nodesource.com/setup_18.x | sudo -E bash -
!sudo apt-get install -y nodejs

!npm info tweet-harvest

data = "111.csv"
search_keyword = "pendidikan since:2024-07-01 until:2024-12-28"
limit = 1000
!npx --yes tweet-harvest@2.6.1 -o "{data}" -s "{search_keyword}" -l {limit} --token ""
# !pip install supabase postgrest pandas

# import pandas as pd
# from supabase import create_client, Client
# import json

# # Supabase credentials
# SUPABASE_URL = "https://riapbavbospmdsbiwnew.supabase.co"
# SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InJpYXBiYXZib3NwbWRzYml3bmV3Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3MzcxMjQwNzQsImV4cCI6MjA1MjcwMDA3NH0.fqq4DOPwFkSdIGgI-jv_DQLT4MYWe4M6IfwAQSfWpOg"

# # Create Supabase client
# supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# # Step 1: Run tweet-harvest command
# data = "111.csv"
# search_keyword = "pendidikan since:2024-07-01 until:2024-12-28"
# limit = 1000

# # Run the tweet-harvest tool
# !npx --yes tweet-harvest@2.6.1 -o "{data}" -s "{search_keyword}" -l {limit} --token ""

# # Step 2: Read and clean the resulting CSV file
# csv_path = f"/content/tweets-data/{data}"
# tweets_df = pd.read_csv(csv_path)

# # Match with Supabase table schema, excluding the 'use' column
# expected_columns = [
#     "conversation_id_str", "created_at", "favorite_count", "full_text", "id_str",
#     "image_url", "in_reply_to_screen_name", "lang", "location", "quote_count",
#     "reply_count", "retweet_count", "tweet_url", "user_id_str"
# ]

# # Filter only existing columns in the CSV
# existing_columns = [col for col in expected_columns if col in tweets_df.columns]
# tweets_df = tweets_df[existing_columns]

# # Ensure data types match the schema
# if "created_at" in tweets_df.columns:
#     tweets_df["created_at"] = pd.to_datetime(tweets_df["created_at"], errors='coerce')  # Convert to datetime
#     tweets_df["created_at"] = tweets_df["created_at"].dt.strftime("%Y-%m-%d %H:%M:%S")  # Convert to string format

# tweets_df = tweets_df.dropna(subset=["id_str", "created_at"])  # Remove rows with critical missing data

# # Step 3: Convert data to dictionary format for insertion
# tweets_data = tweets_df.to_dict(orient="records")

# # Validate and debug data
# for record in tweets_data:
#     print(json.dumps(record, indent=2))  # Debug: Print each record

# # Step 4: Insert data into Supabase
# for tweet in tweets_data:
#     try:
#         cleaned_tweet = {k: v for k, v in tweet.items() if pd.notnull(v) and v != ""}
#         response = supabase.table("twitter_data").insert(cleaned_tweet).execute()

#         if "status_code" in response and response["status_code"] != 201:
#             print(f"Failed to insert: {cleaned_tweet.get('id_str', 'unknown')}, Error: {response}")
#     except Exception as e:
#         print(f"Error inserting data: {e}")

# # Step 5: Save the cleaned CSV to a new file
# output_path = "/content/tweets-data/111_cleaned.csv"
# tweets_df.to_csv(output_path, index=False)

from google.colab import files
files.download('/content/tweets-data/111.csv')

data = "222.csv"
search_keyword = "pendidikan since:2024-07-01 until:2024-12-29"
limit = 1000
!npx --yes tweet-harvest@2.6.1 -o "{data}" -s "{search_keyword}" -l {limit} --token ""

from google.colab import files
files.download('/content/tweets-data/222.csv')

data = "ghi.csv"
search_keyword = "pendidikan since:2024-01-01 until:2024-12-30"
limit = 1000
!npx --yes tweet-harvest@2.6.1 -o "{data}" -s "{search_keyword}" -l {limit} --token ""

from google.colab import files
files.download('/content/tweets-data/ghi.csv')

data = "jkl.csv"
search_keyword = "pendidikan since:2024-07-01 until:2024-12-31"
limit = 1000
!npx --yes tweet-harvest@2.6.1 -o "{data}" -s "{search_keyword}" -l {limit} --token ""

from google.colab import files
files.download('/content/tweets-data/jkl.csv')

data = "m.csv"
search_keyword = "pendidikan since:2024-07-01 until:2024-09-30"
limit = 1000
!npx --yes tweet-harvest@2.6.1 -o "{data}" -s "{search_keyword}" -l {limit} --token ""

from google.colab import files
files.download('/content/tweets-data/m.csv')

data = "n.csv"
search_keyword = "pendidikan since:2024-07-01 until:2024-11-02"
limit = 1000
!npx --yes tweet-harvest@2.6.1 -o "{data}" -s "{search_keyword}" -l {limit} --token ""

from google.colab import files
files.download('/content/tweets-data/n.csv')

data = "o.csv"
search_keyword = "pendidikan since:2024-07-01 until:2024-11-03"
limit = 1000
!npx --yes tweet-harvest@2.6.1 -o "{data}" -s "{search_keyword}" -l {limit} --token ""

from google.colab import files
files.download('/content/tweets-data/o.csv')

data = "p.csv"
search_keyword = "pendidikan since:2024-07-01 until:2024-11-04"
limit = 1000
!npx --yes tweet-harvest@2.6.1 -o "{data}" -s "{search_keyword}" -l {limit} --token ""

from google.colab import files
files.download('/content/tweets-data/p.csv')

data = "q.csv"
search_keyword = "pendidikan since:2024-07-01 until:2024-11-05"
limit = 1000
!npx --yes tweet-harvest@2.6.1 -o "{data}" -s "{search_keyword}" -l {limit} --token ""

import pandas as pd

# List of file paths to combine
file_paths = [
    '/content/tweets-data/111.csv',
    '/content/tweets-data/222.csv',
    '/content/tweets-data/ghi.csv',
    '/content/tweets-data/jkl.csv',
    '/content/tweets-data/m.csv',
    '/content/tweets-data/n.csv',
    '/content/tweets-data/o.csv',
    '/content/tweets-data/p.csv',
    '/content/tweets-data/q.csv'
]

# Combine all files into a single DataFrame
combined_df = pd.concat([pd.read_csv(file) for file in file_paths])

# Rename the column full_text to description
if 'full_text' in combined_df.columns:
    combined_df.rename(columns={'full_text': 'description'}, inplace=True)

# Save the combined DataFrame to a new CSV file
output_path = '/content/tweets-data/all.csv'
combined_df.to_csv(output_path, index=False)

print(f"Combined file with renamed column saved as {output_path}")

# Install dependencies
!pip install supabase postgrest pandas

import pandas as pd
from supabase import create_client, Client
import json
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
import gensim
from gensim import corpora
from gensim.models import LdaModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation

import nltk
# Unduh dependensi NLTK
# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('wordnet')

# # Stop words configuration
# stop_words = stopwords.words('indonesian')
# custom_stop_words = ['yang', 'ga', 'https', 'co', 'yg', 't', 'ya', 'nya', 'gue', 'gw', 'gua', 'aja', 'kalo', 'ga', 'pemerintahnaikkangajiguru', 'amp', 'sih']  # Tambahkan stop words tambahan
# stop_words.extend(custom_stop_words)

# # Tokenizer dan Lemmatizer
# wtk = nltk.tokenize.RegexpTokenizer(r'\w+')
# wnl = WordNetLemmatizer()

# # Gunakan file CSV gabungan
# file_path = '/content/tweets-data/all.csv'
# output_path = '/content/tweets-data/all_cleaned.csv'
# app_reviews_df = pd.read_csv(file_path)

# # Periksa data awal
# print("First 5 rows of the dataset:")
# print(app_reviews_df.head())

# # Gunakan kolom 'description'
# if 'description' in app_reviews_df.columns:
#     reviews = app_reviews_df['description']
# else:
#     raise KeyError("The column 'description' is not found in the dataset. Please check the column names.")

# # Drop nilai NaN
# reviews.dropna(inplace=True)
# reviews.reset_index(drop=True, inplace=True)

# print("\nNumber of reviews after cleaning:")
# print(len(reviews))

# # Fungsi pembersihan
# def clean_text(text):
#     text = text.lower()  # Konversi ke huruf kecil
#     text = re.sub(r'http\S+', '', text)  # Hapus URL
#     text = re.sub(r'\d+', '', text)  # Hapus angka
#     tokens = wtk.tokenize(text)  # Tokenisasi
#     tokens = [word for word in tokens if word not in stop_words]  # Hapus stop words
#     tokens = [wnl.lemmatize(word) for word in tokens]  # Lemmatization
#     return ' '.join(tokens)

# # Bersihkan data
# app_reviews_df['cleaned_description'] = reviews.apply(clean_text)

# # Simpan hasil ke file CSV baru
# app_reviews_df.to_csv(output_path, index=False)

# print(f"Cleaned data saved to: {output_path}")

# # Menampilkan data yang sudah dibersihkan
# print("\nFirst 5 rows of the cleaned dataset:")
# print(app_reviews_df[['description', 'cleaned_description']].head())
# Unduh dependensi NLTK
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Stop words configuration
stop_words = stopwords.words('indonesian')
custom_stop_words = ['yang', 'ga', 'https', 'co', 'yg', 't', 'ya', 'nya', 'gue', 'gw', 'gua', 'aja', 'kalo', 'ga', 'pemerintahnaikkangajiguru', 'amp', 'sih']  # Custom stopwords
stop_words.extend(custom_stop_words)

# Tokenizer dan Lemmatizer
wtk = nltk.tokenize.RegexpTokenizer(r'\w+')
wnl = WordNetLemmatizer()

# Supabase credentials
SUPABASE_URL = "https://riapbavbospmdsbiwnew.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InJpYXBiYXZib3NwbWRzYml3bmV3Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3MzcxMjQwNzQsImV4cCI6MjA1MjcwMDA3NH0.fqq4DOPwFkSdIGgI-jv_DQLT4MYWe4M6IfwAQSfWpOg"

# Create Supabase client
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# File paths
file_path = '/content/tweets-data/all.csv'
output_path = '/content/tweets-data/all_cleaned.csv'

# Step 1: Load the original data
app_reviews_df = pd.read_csv(file_path)

# Periksa data awal
print("First 5 rows of the dataset:")
print(app_reviews_df.head())

# Gunakan kolom 'description'
if 'description' in app_reviews_df.columns:
    reviews = app_reviews_df['description']
else:
    raise KeyError("The column 'description' is not found in the dataset. Please check the column names.")

# Drop nilai NaN
reviews.dropna(inplace=True)
reviews.reset_index(drop=True, inplace=True)

print("\nNumber of reviews after cleaning:")
print(len(reviews))

# Fungsi pembersihan
def clean_text(text):
    text = text.lower()  # Konversi ke huruf kecil
    text = re.sub(r'http\S+', '', text)  # Hapus URL
    text = re.sub(r'\d+', '', text)  # Hapus angka
    tokens = wtk.tokenize(text)  # Tokenisasi
    tokens = [word for word in tokens if word not in stop_words]  # Hapus stop words
    tokens = [wnl.lemmatize(word) for word in tokens]  # Lemmatization
    return ' '.join(tokens)

# Bersihkan data
app_reviews_df['cleaned_description'] = reviews.apply(clean_text)

# Simpan hasil ke file CSV baru
app_reviews_df.to_csv(output_path, index=False)

print(f"Cleaned data saved to: {output_path}")

# Menampilkan data yang sudah dibersihkan
print("\nFirst 5 rows of the cleaned dataset:")
print(app_reviews_df[['description', 'cleaned_description']].head())

# Step 2: Insert cleaned data into Supabase
# Ensure necessary columns are present
required_columns = [
    "conversation_id_str", "created_at", "favorite_count", "description", "id_str",
    "image_url", "in_reply_to_screen_name", "lang", "location", "quote_count",
    "reply_count", "retweet_count", "tweet_url", "user_id_str"
]

# Load the cleaned data file
app_reviews_df = pd.read_csv(output_path)

if not set(required_columns).issubset(app_reviews_df.columns):
    raise KeyError("The required columns are not present in the dataset.")

# Ensure correct data types for 'created_at'
app_reviews_df["created_at"] = pd.to_datetime(app_reviews_df["created_at"], errors='coerce')  # Convert to datetime
app_reviews_df["created_at"] = app_reviews_df["created_at"].dt.strftime("%Y-%m-%d %H:%M:%S")  # Convert to string

# Use 'cleaned_description' column for insertion into 'description' column
app_reviews_df["description"] = app_reviews_df["cleaned_description"]

# Convert data to dictionary for insertion
tweets_data = app_reviews_df.to_dict(orient="records")

# Step 3: Insert data into Supabase
for tweet in tweets_data:
    try:
              # Remove any null or invalid values
        cleaned_tweet = {k: v for k, v in tweet.items() if pd.notnull(v) and v != ""}

        # Insert cleaned data into Supabase
        response = supabase.table("twitter_data").insert(cleaned_tweet).execute()

        # Check response status
        if "status_code" in response and response["status_code"] != 201:
            print(f"Failed to insert: {cleaned_tweet.get('id_str', 'unknown')}, Error: {response}")
    except Exception as e:
        print(f"Error inserting data: {e}")

print("Data insertion to Supabase completed.")

def preprocess_text(text):
    tokens = wtk.tokenize(text.lower())
    filtered_tokens = [word for word in tokens if word not in stop_words]
    lemmatized_tokens = [wnl.lemmatize(word) for word in filtered_tokens]
    return " ".join(lemmatized_tokens)

processed_reviews = reviews.apply(preprocess_text)

print(processed_reviews.head())

from gensim import corpora
from gensim.models import LdaModel

processed_reviews_list = processed_reviews.tolist()
dictionary = corpora.Dictionary([review.split() for review in processed_reviews_list])
corpus = [dictionary.doc2bow(review.split()) for review in processed_reviews_list]

num_topics = 5

lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10, random_state=42)

topics = lda_model.show_topics(num_topics=num_topics, num_words=7, formatted=False)
topics = sorted(topics, key=lambda x: x[0])

for i, (_, word_list) in enumerate(topics):
    topic_words = ', '.join([word for word, _ in word_list])
    print(f"Topic {i}: {topic_words}")